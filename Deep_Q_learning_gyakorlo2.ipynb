{
  "cells": [
    {
      "cell_type": "raw",
      "id": "ec0d31c9-fe32-46f5-8e00-1b68cd45ac03",
      "metadata": {
        "id": "ec0d31c9-fe32-46f5-8e00-1b68cd45ac03"
      },
      "source": [
        "Lunar lander\n",
        "\n",
        "\"\"\"\n",
        "box2d do not support windows. (try linux)\n",
        "for some reason installing it with pip, leads to error. (wheels error).\n",
        "but ppl say, it works on windows with anaconda https://github.com/openai/gym/issues/3143\n",
        "\n",
        "conda install swig\n",
        "conda install gymnasium[box2d]\n",
        "\"\"\"\n",
        "\n",
        "https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
        "1.)according to description:\n",
        "4 possible action (action space):\n",
        "    0: do nothing\n",
        "    1: fire left orientation engine\n",
        "    2: fire main engine\n",
        "    3: fire right orientation engine\n",
        "state (observation space): \"8 dimensional vector\" -> 8 input in a row (x&y coordinates of the lander, x&y velicity, angle, angular velocity, 2 booleans for each leg on the ground or not)\n"
      ]
    },
    {
      "cell_type": "raw",
      "id": "a3434bfd-9e0a-43b4-ae34-6d166134440a",
      "metadata": {
        "id": "a3434bfd-9e0a-43b4-ae34-6d166134440a"
      },
      "source": [
        "plan:\n",
        "    up to down -> start from the environment (because its static) and from training step\n",
        "    imports\n",
        "    create network\n",
        "    init hyperparameters\n",
        "    create experience replay\n",
        "    create agent\n",
        "    init environment, agent\n",
        "    train agent\n",
        "    visualize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is an implementation of the gymnasium Lunar Lander ML project with torch.\n",
        "\n",
        "In this project, the AI(rather MI) learns how to land a space shuttle on the moon in a 2d simulated space environment.\n",
        "\n",
        "link: https://gymnasium.farama.org/environments/box2d/lunar_lander/\n"
      ],
      "metadata": {
        "id": "v8Qv8fK6VWLW"
      },
      "id": "v8Qv8fK6VWLW"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "box2d do not support windows. (try linux)\n",
        "for some reason installing it with pip, leads to error. (wheels error).\n",
        "but ppl say, it works on windows with anaconda https://github.com/openai/gym/issues/3143\n",
        "\n",
        "conda install swig\n",
        "conda install gymnasium[box2d]\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "n7zWhu16UemM"
      },
      "id": "n7zWhu16UemM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# installing dependencies for google colab.\n",
        "# see requirements.txt, if you do not use colab.\n",
        "\n",
        "!pip install gymnasium\n",
        "!pip install \"gymnasium[atari, accept-rom-license]\"\n",
        "!apt-get install -y swig\n",
        "!pip install gymnasium[box2d]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J75CfJRoW8ws",
        "outputId": "65fd5272-3df0-45d6-dbe7-92ca0fa2b5d8"
      },
      "id": "J75CfJRoW8ws",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.10.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: gymnasium[accept-rom-license,atari] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (4.10.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n",
            "Collecting shimmy[atari]<1.0,>=0.1.0 (from gymnasium[accept-rom-license,atari])\n",
            "  Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
            "Collecting autorom[accept-rom-license]~=0.4.2 (from gymnasium[accept-rom-license,atari])\n",
            "  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (4.66.2)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari])\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0->gymnasium[accept-rom-license,atari])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0->gymnasium[accept-rom-license,atari]) (6.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gymnasium[accept-rom-license,atari]) (2024.2.2)\n",
            "Building wheels for collected packages: AutoROM.accept-rom-license\n",
            "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446659 sha256=af9ef396e2bc9ebb828c7f734ef2db56d4414a9343588e7d216a06dfd9ef486b\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
            "Successfully built AutoROM.accept-rom-license\n",
            "Installing collected packages: ale-py, shimmy, AutoROM.accept-rom-license, autorom\n",
            "Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.4.2 shimmy-0.2.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 0s (3,883 kB/s)\n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 121753 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.10.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.5.2)\n",
            "Collecting swig==4.* (from gymnasium[box2d])\n",
            "  Using cached swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2349114 sha256=40db197881ef71094e8b23187ac1afe144e3dfa66fff49763172aff325ff73c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: swig, box2d-py\n",
            "Successfully installed box2d-py-2.3.5 swig-4.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9d380ed-7a37-4817-a234-d69d3f4de265",
      "metadata": {
        "id": "c9d380ed-7a37-4817-a234-d69d3f4de265"
      },
      "source": [
        "# 1. imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "97d49609-1bd8-465d-a679-37cfb720bcea",
      "metadata": {
        "id": "97d49609-1bd8-465d-a679-37cfb720bcea"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "384be25b-b8d3-4855-97a7-cd9bc8420f54",
      "metadata": {
        "id": "384be25b-b8d3-4855-97a7-cd9bc8420f54"
      },
      "source": [
        "# 2. create network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ff77520d-b3d0-432a-bb9f-aa608133b033",
      "metadata": {
        "id": "ff77520d-b3d0-432a-bb9f-aa608133b033"
      },
      "outputs": [],
      "source": [
        "# !!!! in this example we do not scale the input datas (-> already mostly in the same scale)\n",
        "class Network(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super().__init__()\n",
        "        self.fcl1 = nn.Linear(state_size, 64)\n",
        "        self.fcl2 = nn.Linear(64, 64)\n",
        "        self.fcl3 = nn.Linear(64, action_size)\n",
        "\n",
        "\n",
        "    def forward(self, state):\n",
        "        signal = self.fcl1(state)\n",
        "        signal = F.relu(signal)\n",
        "        signal = self.fcl2(signal)\n",
        "        signal = F.relu(signal)\n",
        "        return self.fcl3(signal)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75e23bce-0441-4c49-9e3d-6201ce5c5df0",
      "metadata": {
        "id": "75e23bce-0441-4c49-9e3d-6201ce5c5df0"
      },
      "source": [
        "# 3 init hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c6f98e8f-d260-4173-9a7a-2f894e006cd6",
      "metadata": {
        "id": "c6f98e8f-d260-4173-9a7a-2f894e006cd6"
      },
      "outputs": [],
      "source": [
        "memory_size = 1e5 # -> 10^5=100.000\n",
        "learning_batch_size = 100\n",
        "alpha = 0.0005 # learning rate\n",
        "gamma = 0.99 # discount factor\n",
        "interpolation_parameter = 0.001 # how much information will be incorporated from the local network to the target network"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c10479ae-af53-4802-b13b-8bfb277b65b4",
      "metadata": {
        "id": "c10479ae-af53-4802-b13b-8bfb277b65b4"
      },
      "source": [
        "# 4 create experience replay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "60d18780-90a3-4a27-802b-b099b4612cf8",
      "metadata": {
        "id": "60d18780-90a3-4a27-802b-b099b4612cf8"
      },
      "outputs": [],
      "source": [
        "class ReplayMemory():\n",
        "\n",
        "\n",
        "    def __init__(self, memory_size, learning_batch_size):\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.capacity = memory_size\n",
        "        self.learning_batch_size = learning_batch_size\n",
        "        self.memory = []\n",
        "\n",
        "\n",
        "    def push(self, event):\n",
        "        # fifo behavior -> first in first out\n",
        "        self.memory.append(event)\n",
        "        if len(self.memory) > self.capacity:\n",
        "            del self.memory[0]\n",
        "\n",
        "\n",
        "    # sample and transform\n",
        "    def sample(self, sample_size=None):\n",
        "        # can't use: sample_size=self.learning_batch_size -> because in python default values are executed when the function is defined,\n",
        "        # which doesn't exists now. this is a common technik to create this behavior.\n",
        "        if sample_size == None:\n",
        "            sample_size = self.learning_batch_size\n",
        "        # dtypes in memory (returned dtype from env+action): np.array, np.int64, float, bool, bool, sett\n",
        "        experiences = random.sample(self.memory, k=sample_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e[0] for e in experiences if e is not None])).float().to(self.device)\n",
        "        actions = torch.from_numpy(np.vstack([e[1] for e in experiences if e is not None])).long().to(self.device)\n",
        "        rewards = torch.from_numpy(np.vstack([e[2] for e in experiences if e is not None])).float().to(self.device)\n",
        "        next_states = torch.from_numpy(np.vstack([e[3] for e in experiences if e is not None])).float().to(self.device)\n",
        "        dones = torch.from_numpy(np.vstack([e[4] for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
        "        return states, actions, rewards, next_states, dones\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd3284b9-e195-42d8-b036-43c92cdb16f0",
      "metadata": {
        "id": "cd3284b9-e195-42d8-b036-43c92cdb16f0"
      },
      "source": [
        "# 5. create the agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "b5f3c0d5-195b-482e-a738-3145c02c7615",
      "metadata": {
        "id": "b5f3c0d5-195b-482e-a738-3145c02c7615"
      },
      "outputs": [],
      "source": [
        "class Agent():\n",
        "\n",
        "\n",
        "    def __init__(self, state_size, action_size, memory_size, learning_batch_size, learning_rate, discount_factor, interpolation_parameter):\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory_size = memory_size\n",
        "        self.learning_batch_size = learning_batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.interpolation_parameter = interpolation_parameter\n",
        "        self.memory = ReplayMemory(memory_size, learning_batch_size)\n",
        "        self.local_qnetwork = Network(state_size, action_size).to(self.device)\n",
        "        self.target_qnetwork = Network(state_size, action_size).to(self.device)\n",
        "        self.optimizer = Adam(self.local_qnetwork.parameters(), lr=learning_rate)\n",
        "        self.t_step = 0\n",
        "\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        self.memory.push((state, action, reward, next_state, done))\n",
        "\n",
        "        self.t_step = (self.t_step + 1) % 4\n",
        "        if self.t_step == 0:\n",
        "            if len(self.memory.memory) > self.learning_batch_size:\n",
        "                experiences = self.memory.sample(self.learning_batch_size)\n",
        "                self.learn(experiences)\n",
        "\n",
        "\n",
        "    # epsilon greedy policy\n",
        "    def act(self, state, epsilon):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
        "        self.local_qnetwork.eval()\n",
        "        with torch.no_grad():\n",
        "            action_values = self.local_qnetwork(state).data\n",
        "            #print(\"act: action values: \", action_values)\n",
        "        if random.random() > epsilon:\n",
        "            action = np.argmax(action_values.cpu().numpy())\n",
        "            #print(\"act: best action\", action)\n",
        "        else:\n",
        "            action = random.choice(np.arange(action_size, dtype=np.int64))\n",
        "            #print(\"act: action random\", action)\n",
        "        #print(\"type of action: \", type(action))\n",
        "        return action\n",
        "\n",
        "\n",
        "    def learn(self, experiences):\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "        expected_q_values = self.target_qnetwork(next_states).detach()\n",
        "        #print(\"learn: expected_q_values: \", expected_q_values)\n",
        "        expected_best_q_value = expected_q_values.max(1)[0].unsqueeze(1)\n",
        "        #print(\"learn: expected_best_q_value: \", expected_best_q_value)\n",
        "        q_targets = rewards + self.discount_factor * expected_best_q_value * (1 - dones)\n",
        "        predicted_q_values = self.local_qnetwork(states).gather(1, actions)\n",
        "        loss = F.mse_loss(predicted_q_values, q_targets)\n",
        "        #print(\"learn: loss: \", loss)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.soft_update(self.local_qnetwork, self.target_qnetwork, interpolation_parameter)\n",
        "\n",
        "\n",
        "    # nn.module.parameters() returns an iterator, which is reference of its tensor content -> modifying iterator modify the tensor\n",
        "    # zip() returns with its original input values + types -> tensors -> modifying the target_param directly modify the network object\n",
        "    def soft_update(self, local_model, target_model, interpolation_parameter):\n",
        "        for local_params, target_params in zip(local_model.parameters(), target_model.parameters()):\n",
        "            weighted_params = interpolation_parameter * local_params.data + (1.0 - interpolation_parameter) * target_params.data\n",
        "            target_params.data = weighted_params\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4094399-e9bc-41f2-bce5-505c3488f1ea",
      "metadata": {
        "id": "d4094399-e9bc-41f2-bce5-505c3488f1ea"
      },
      "source": [
        "# 6. init environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3ea199f8-86e1-4894-89d1-e485a374ab81",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ea199f8-86e1-4894-89d1-e485a374ab81",
        "outputId": "60c1c582-8802-4d46-8217-6d79eb0d555f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "state size:  8\n",
            "action size:  4\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "\n",
        "env = gym.make(\"LunarLander-v2\")\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "print(\"state size: \", state_size)\n",
        "print(\"action size: \", action_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9c152958-ac44-419a-9fb4-d3eccccb7f07",
      "metadata": {
        "id": "9c152958-ac44-419a-9fb4-d3eccccb7f07"
      },
      "outputs": [],
      "source": [
        "# init agent\n",
        "\n",
        "agent = Agent(state_size, action_size, memory_size, learning_batch_size, alpha, gamma, interpolation_parameter)"
      ]
    },
    {
      "cell_type": "raw",
      "id": "7e803e30-8fae-4cee-bd7a-f8d963db9b32",
      "metadata": {
        "id": "7e803e30-8fae-4cee-bd7a-f8d963db9b32"
      },
      "source": [
        "# TESTS\n",
        "# agent.act() test\n",
        "\n",
        "dummy_state = np.array([-1.5, -1.5, -5., -5., -3.1415927, -5., -0., -0. ])\n",
        "dummy_epsilon = 0.5\n",
        "action = agent.act(dummy_state, dummy_epsilon)\n",
        "print(\"action: \", action)\n",
        "print(\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c828cc5c-860d-498e-b23a-1dde5fb46adc",
      "metadata": {
        "id": "c828cc5c-860d-498e-b23a-1dde5fb46adc"
      },
      "source": [
        "# 7. training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "9d1d0326-b5c6-4306-92b2-532b85ecee9c",
      "metadata": {
        "id": "9d1d0326-b5c6-4306-92b2-532b85ecee9c"
      },
      "outputs": [],
      "source": [
        "# training parameters\n",
        "\n",
        "number_episodes = 1600\n",
        "max_steps_per_episode = 1000\n",
        "epsilon_start = 0.99\n",
        "epsilon_end = 0.01\n",
        "epsilon_decay = 0.997\n",
        "epsilon = epsilon_start\n",
        "score_on_100_episode = deque(maxlen=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "71d4bdb0-b4b6-4b4d-b77e-9905ce9e127a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "71d4bdb0-b4b6-4b4d-b77e-9905ce9e127a",
        "outputId": "fc72ff68-195e-4bc8-d2a6-9fa7ebb53f36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Episode: 100 \t Average_score: -160.20\n",
            " Episode: 200 \t Average_score: -113.81\n",
            " Episode: 296 \t Average_score: -94.05"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-19ac9c052dc9>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# returned dtypes: np.array, float, bool, bool, sett\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-5b9dec923475>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_batch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-046b975e31a6>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, sample_size)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcasting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# training\n",
        "\n",
        "for episode in range(1, number_episodes + 1):\n",
        "    score = 0\n",
        "    state, _ = env.reset()\n",
        "    for step in range(max_steps_per_episode):\n",
        "        action = agent.act(state, epsilon)\n",
        "        # returned dtypes: np.array, float, bool, bool, sett\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "        agent.step(state, action, reward, next_state, done)\n",
        "        score += reward\n",
        "        state = next_state\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    score_on_100_episode.append(score)\n",
        "    epsilon = max(epsilon * epsilon_decay, epsilon_end)\n",
        "\n",
        "\n",
        "    # visualize\n",
        "    # \"\\r\" ensures that each new print statement overwrites the previous one on the same line,\n",
        "    # creating a dynamic display of information, commonly used in console-based progress indicators or animations (+must remove the default end).\n",
        "    print(\"\\r Episode: {} \\t Average_score: {:.2f}\".format(episode, np.mean(score_on_100_episode)), end=\"\")\n",
        "    # keep every 100 episode on the screen.\n",
        "    if episode % 100 == 0:\n",
        "        print(\"\\r Episode: {} \\t Average_score: {:.2f}\".format(episode, np.mean(score_on_100_episode)))\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5ae8e37-5c93-441c-84ca-329db9fc783c",
      "metadata": {
        "id": "b5ae8e37-5c93-441c-84ca-329db9fc783c"
      },
      "outputs": [],
      "source": [
        "# copy code: video visualize\n",
        "\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import imageio\n",
        "from IPython.display import HTML, display\n",
        "from gymnasium.wrappers.monitoring.video_recorder import VideoRecorder\n",
        "\n",
        "def show_video_of_model(agent, env_name):\n",
        "    env = gym.make(env_name, render_mode='rgb_array')\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    frames = []\n",
        "    while not done:\n",
        "        frame = env.render()\n",
        "        frames.append(frame)\n",
        "        action = agent.act(state, epsilon)\n",
        "        state, reward, done, _, _ = env.step(action.item())\n",
        "    env.close()\n",
        "    imageio.mimsave('video.mp4', frames, fps=30)\n",
        "\n",
        "show_video_of_model(agent, 'LunarLander-v2')\n",
        "\n",
        "def show_video():\n",
        "    mp4list = glob.glob('*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "        display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print(\"Could not find video\")\n",
        "\n",
        "show_video()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d7b3daf-c622-4601-96b0-ffa32ba572aa",
      "metadata": {
        "id": "1d7b3daf-c622-4601-96b0-ffa32ba572aa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}